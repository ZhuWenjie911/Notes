问题：状态空间过大，agent难以探索到实现目标的episode，造成正样本数据量过小，难以训练policy或训练失败。
解决方法包括但不限于：
  1.reward shaping：通过人为设计稠密连续的奖励来替代只有最后一步的奖励
  2.in_reward & ex_reward: ex_reward为达到目标的最终奖励，但中间过程中，添加in_reward来引导智能体往目标前进
  3.逆强化学习：拟合出一个奖励函数
  4.Hindsight Experience Replay（HER）：一般的强化学习方法对于无奖励的样本几乎没有利用，HER的思想就是从无奖励的样本中学习。
  HER建立在多目标强化学习的基础上，将失败的状态映射为新的目标  ，使用替换原目标  就得到了一段“成功”的经历（达到了目标）
  5.Curiosity Driven：好奇心驱动是使用内在奖励鼓励agent探索更陌生的状态，平衡探索与利用，本质上是提高了样本的利用效率，
  主要分为两类，分别是基于状态计数的和基于状态预测误差的方法
  6.Imitation Learning：对专家策略进行学习
  7.Curriculum Learning：通过设置不同难度梯度的课程来加速学习，类似人类学习的过程，从简单的问题学习到的策略能够迁移到复杂的问题中。

